# pecan
오픈소스 5
1. 주제
차선 이탈 및 차량 밀집도 기반 신호등 자동 조정 알고리즘
분반, 팀, 학번, 이름
나분반, 5팀, 20251762, 박준서
2. 요약 
본 프로젝트의 목표는 자율주행 시스템이 신호등, 차선 등의 정형화된 교통 정보 외에, 교통경찰의 수신호와 같은 비정형적 시각 정보를 인식하지 못하는 한계를 극복하는 것이다. Ubuntu 기반의 오픈소스 환경에서 컴퓨터 비전과 딥러닝 기술을 융합하여 교통경찰의 팔 동작과 자세를 실시간으로 추출하고, 이를 '정지', '진행', '좌회전', '우회전'과 같은 차량 제어 명령으로 변환하는 시스템을 구축하는 것이다. 구체적으로는 OpenCV로 영상 전처리, MediaPipe/OpenPose로 포즈 추출, TensorFlow/Keras 기반 분류 모델로 제스처 분류, 그리고 ROS/Python API로 제어 신호를 전달하는 시스템 아키텍처를 제시한다. 이 프로젝트의 중요성은 교통신호 고장, 행사, 긴급 상황 등에서 교통경찰의 수동 제어에 자율주행차나 운전자 보조 시스템이 안전하게 대응할 수 있는 기반 기술을 마련하여, 자율주행의 안전성을 향상시키고 인간 중심의 교통 환경과 조화롭게 공존할 수 있는 지능 교통체계의 핵심 구성요소로 발전할 수 있다는 점이다
4. 서론
1. 연구 배경
최근 자율주행자동차는 딥러닝 기반 객체 인식과 센서 융합 기술(LiDAR, 카메라, 레이더 등)을 통해 빠르게 상용화되고 있다. 그러나 대부분의 인식 시스템은 신호등, 차선, 표지판 등 정형화된 교통 인프라 정보에 최적화되어 있다. 반면, 교통경찰의 수신호 제스처는 사람의 신체 움직임, 팔의 방향, 정지·진행 동작 등과 같은 비정형적 시각 정보로 구성되어 있어, 이를 정확히 인식하기 어렵다. 
이러한 상황은 교통신호 고장, 전력 공급 문제, 이벤트·행사로 인한 도로 통제, 응급상황 발생 시 수동 교통 유도 등에서 빈번하게 발생한다. 실제 도심 교차로에서는 교통경찰이 차량 흐름을 수동으로 제어하는 경우가 많으며, 이때 자율주행 시스템이 이를 인식하지 못하면 교통 혼란, 안전사고, 비상 정지 등 위험한 결과로 이어질 수 있다. 따라서 교통경찰의 제스처를 기계가 이해할 수 있는 형태로 인식·분류하는 기술은 자율주행 안전성 향상을 위한 핵심 과제라 할 수 있다.
2. 사례 분석
현재 상용 자율주행 시스템들은 교통경찰 수신호 인식에 대한 대응이 미흡하다. 예를 들어, Tesla Autopilot이나 Waymo, Hyundai Mobis의 ADAS 시스템 등은 도로 표식 및 신호 인식에는 높은 정확도를 보이지만, 사람의 수신호에 대해서는 대부분 운전자 개입(Manual Override)이 필요하다.
또한, 연구 단계에서는 몇몇 대학 및 기업 연구소에서 MediaPipe, YOLO, OpenPose 등의 오픈소스 모델을 활용하여 제스처 인식 실험을 수행하였지만, 대부분 정적 이미지를 기준으로 한 제한적 환경(조명·각도 고정)에서만 인식 성능을 평가하였다.
이처럼 실제 도심 환경에서 다양한 각도, 조명, 거리에서 제스처를 실시간으로 인식할 수 있는 시스템은 아직 부족한 실정이다. 따라서 본 연구는 실제 교통현장에서의 제스처 인식 안정성을 확보하고, 오픈소스 기반 기술로 누구나 구현 가능한 모델을 제시한다는 점에서 의미가 있다.
3. 문제 정의
현재 자율주행자동차 및 운전자 보조 시스템은 신호등, 차선, 표지판과 같은 정형화된 교통 신호 정보에만 의존하고 있다. 그러나 실제 도심 환경에서는 교통신호 고장, 공사, 행사, 긴급 상황 등으로 인해 교통경찰의 수신호 제어가 우선되는 비정형 상황이 자주 발생한다. 이러한 상황에서 차량이 교통경찰의 제스처를 인식하지 못하면, 오작동이나 교통 혼란을 초래하여 운전자 및 보행자의 안전을 위협할 가능성이 높다. 또한 현재 상용화된 자율주행 시스템은 대부분 교통경찰 제스처 인식 기능을 포함하지 않거나, 단순 정지·진행만 구분하는 제한적인 수준에 머무르고 있다. 조명, 각도, 거리 등의 환경적 변수에도 민감하게 반응하여 실제 도심 환경에서의 실시간 인식률이 낮은 문제 역시 존재한다. 더불어 교통경찰 수신호 영상 데이터셋이 부족하여 학습용 데이터 확보가 어렵고, 모델의 일반화 성능이 떨어지는 한계도 있다. 결국 현재의 자율주행 시스템은 인간 교통경찰의 신호를 이해하지 못한다는 점에서, 완전 자율주행 단계로 나아가기 위한 핵심적인 인식 공백이 존재한다고 할 수 있다.
4. 극복 방안
이러한 문제를 해결하기 위해 본 연구는 Ubuntu 기반 오픈소스 환경에서 동작하는 교통경찰 제스처 인식 시스템을 개발하고자 한다. 시스템은 컴퓨터 비전과 딥러닝 기술을 융합하여 교통경찰의 팔 동작, 신체 자세, 방향 등을 실시간으로 추출하고, 이를 정지·진행·좌회전·우회전 등 차량 제어 명령으로 변환하는 것을 목표로 한다. 구체적으로는 OpenCV를 활용해 영상 전처리 및 프레임 추출을 수행하고, MediaPipe 또는 OpenPose를 이용해 인체의 주요 관절 좌표를 감지한다. 이후 TensorFlow/Keras 기반 분류 모델을 통해 각 제스처를 학습·판별하며, 조명·거리·시야각 변화에 대응하기 위해 데이터 증강(Data Augmentation) 기법을 적용한다.
개발 환경은 Ubuntu를 기반으로 하여 vim을 통한 경량 코드 작성, git을 통한 버전 관리 및 협업 구조를 도입함으로써, 효율적이고 재현 가능한 오픈소스 개발 방식을 구현한다. 이 시스템이 완성되면 자율주행차나 운전자 보조 시스템이 교통경찰의 수신호를 인식해 비정형 상황에서도 안전하게 주행 판단을 내릴 수 있는 기반 기술로 발전할 수 있을 것이다.
5. 본론
  

1.	필요 기술 요소 및 구현 방법
본 프로젝트의 핵심은 교통경찰 제스처를 실시간으로 인식하고 자율주행 차량의 주행 판단에 반영하는 인공지능 기반 시스템을 구축하는 것이다. 이를 위해서는 영상처리, 포즈 추출, 제스처 분류, 제어 신호 변환 등 여러 기술 요소가 유기적으로 결합되어야 한다.
첫째, 영상 입력 및 전처리(Video Input & Pre-processing) 단계에서는 차량에 탑재된 카메라를 통해 수집한 실시간 영상을 처리한다. 조도 변화, 날씨, 거리 등의 외부 요인에 따라 영상 품질이 달라지므로, OpenCV 라이브러리를 이용해 프레임 정규화, 노이즈 제거, 명암 대비 보정, ROI(Region of Interest) 추출 등의 전처리 과정을 수행한다. 이 과정을 통해 인식 모델이 안정적으로 특징을 추출할 수 있도록 한다.
둘째, 포즈 인식(Pose Estimation) 단계에서는 MediaPipe 또는 OpenPose를 활용하여 교통경찰의 신체 관절 좌표를 검출한다. 각 프레임에서 2D 혹은 3D 키포인트를 추출한 뒤, 이를 벡터 형태로 변환하여 팔, 손, 어깨 등의 상대적 위치 관계를 분석한다. 이 단계는 전체 인식 정확도에 큰 영향을 미치기 때문에, 다양한 각도와 조명 조건에서도 안정적으로 관절을 추출할 수 있는 모델 튜닝 및 데이터 증강(Data Augmentation)이 필요하다.
셋째, 제스처 분류(Gesture Classification) 단계에서는 추출된 포즈 데이터를 기반으로 CNN(Convolutional Neural Network) 또는 LSTM(Long Short-Term Memory) 모델을 학습시켜 각 제스처를 정지·진행·좌회전·우회전 등의 명령으로 분류한다. 학습 과정에서는 TensorFlow/Keras를 사용하며, 모델의 과적합을 방지하기 위해 드롭아웃(Dropout) 및 배치 정규화(Batch Normalization) 기법을 적용한다. 또한 학습 데이터가 부족한 현실적 한계를 보완하기 위해, 자체 촬영 데이터와 공개 제스처 데이터셋을 결합하여 학습 효율을 높인다.
마지막으로, 제어 신호 변환 및 출력(Command Output) 단계에서는 분류된 제스처 정보를 차량 시스템에 전달한다. 이 신호는 ROS(Robot Operating System)나 Python API를 통해 자율주행 제어 모듈에 연동되며, 실제 차량 제동·가속·회전 명령으로 변환된다. 이러한 통합 과정을 통해 시스템은 인간 교통경찰의 신호를 자율주행 판단 로직에 반영할 수 있게 된다.


2.	개발 방향
본 프로젝트의 개발 방향은 오픈소스 기반의 실시간 제스처 인식 시스템 구축에 초점을 맞추고 있다. 개발 환경은 Ubuntu를 기반으로 하며, 이는 다양한 인공지능 라이브러리와 하드웨어 가속 기능을 지원하여 안정적인 학습 및 실험 환경을 제공한다. 코딩 환경으로는 vim을 사용하여 경량화된 개발을 수행하고, git을 통한 버전 관리 및 협업 체계를 구축함으로써 코드의 수정 내역을 체계적으로 관리한다.
기술적으로는 OpenCV, MediaPipe, TensorFlow/Keras 등 오픈소스 라이브러리를 중심으로 시스템을 구성한다. OpenCV를 활용하여 영상 입력과 전처리를 수행하고, MediaPipe나 OpenPose를 통해 교통경찰의 신체 관절 좌표를 실시간으로 추출한다. 이후 추출된 포즈 데이터를 CNN이나 LSTM 기반의 분류 모델로 학습시켜, 정지·진행·좌회전·우회전과 같은 제스처를 판별한다. 이 과정을 통해 교통경찰의 수신호를 차량 제어 신호로 변환하여, 자율주행 시스템이 비정형적 상황에서도 안전하게 반응할 수 있도록 한다.
또한 시스템의 효율성을 높이기 위해 데이터 증강, 모델 경량화, 프레임 최적화 등의 기법을 도입하고, ROS(Robot Operating System) 또는 Python API를 활용하여 자율주행 모듈과의 연동성을 확보한다. 이러한 개발 방향은 자율주행차가 실제 도로 환경에서 인간의 제스처를 이해하고 대응하는 능력을 갖추도록 하는 데 초점을 맞춘 것이다. 나아가, 해당 연구는 완성형 제품보다는 확장 가능한 오픈소스 프로토타입으로서, 다양한 연구자와 개발자가 개선에 참여할 수 있는 기반을 마련하고자 한다.

6. 결론
1. 보고 내용 요약
본 프로젝트는 교통경찰 제스처 인식 시스템을 자율주행차 및 운전자 보조 시스템에 적용하기 위한 기술적 접근을 다룬다.
프로젝트의 출발점은 자율주행 시스템이 신호등·차선·표지판 등 정형화된 교통 정보에는 대응 가능하지만, 실제 도심 환경에서 교통경찰의 비정형적 수신호를 인식하지 못하는 한계를 극복하는 데 있다. 이를 해결하기 위해 본 프로젝트는 오픈소스 기반 인공지능 비전 시스템을 구축하여, 교통경찰의 신체 자세와 팔의 방향을 실시간으로 추출하고 이를 차량 제어 명령으로 변환하는 것을 목표로 하였다.
시스템은 Ubuntu 환경에서 구축되며, OpenCV를 활용한 영상 전처리, MediaPipe/OpenPose를 이용한 포즈 추출, TensorFlow/Keras 기반의 제스처 분류 모델, 그리고 ROS 또는 Python API를 통한 제어 신호 전달로 구성된다. 이러한 구조를 통해 교통경찰의 ‘정지’, ‘진행’, ‘좌회전’, ‘우회전’ 동작을 실시간으로 인식하여 자율주행 차량의 판단 로직에 반영할 수 있다.
또한 개발 전 과정에서 vim을 이용한 경량 코드 관리와 git을 통한 협업 및 버전 관리 체계를 도입하여, 효율적이고 재현 가능한 오픈소스 개발 프로세스를 확립하였다. 본 보고서에서는 시스템 설계의 전체 흐름, 핵심 기술 요소, 구현 방법, 그리고 향후 기술 발전 가능성을 종합적으로 제시하였다. 이로써 자율주행 시스템이 사람의 수신호를 이해하는 단계로 발전할 수 있는 기초 기술적 틀을 제시했다는 점에서 학술적·실용적 의의를 가진다.

2.향후 할 일 및 발전 방향
향후 프로젝트는 현재 구축된 시스템의 정확도와 실용성을 강화하는 데 중점을 둘 예정이다. 우선, 실제 교통경찰의 제스처 영상 데이터셋을 수집하고 구축하는 작업이 선행되어야 한다. 다양한 각도, 조명, 거리, 날씨 조건에서 촬영된 데이터를 확보함으로써, 모델의 학습 다양성을 높이고 실제 환경에서의 인식률을 향상시킬 수 있다. 학습 데이터의 한계를 극복하기 위해서는 데이터 증강(Data Augmentation)과 합성 데이터(Synthetic Data) 생성 기술도 병행 적용할 계획이다.
다음으로, 모델 고도화와 성능 평가가 이루어질 것이다. CNN, LSTM, Transformer 기반 구조를 비교 실험하여 정확도와 처리 속도를 최적화하고, 인식 결과를 FPS(Frame Per Second), 반응 지연 시간, 정확도 등의 지표로 평가한다. 이를 통해 실시간성(real-time performance)을 확보하고, 자율주행 환경에 적합한 모델을 선정한다.
또한 시뮬레이터 연동을 통한 주행 실험을 계획하고 있다. CARLA나 LGSVL과 같은 자율주행 시뮬레이터에서 제스처 인식 결과가 차량의 주행 결정에 미치는 영향을 검증하고, 실제 도심 교차로 상황을 가정한 테스트를 수행할 예정이다. 이후에는 소형 자율주행 차량 플랫폼을 이용한 오프라인 실험 및 실제 주행 테스트를 통해 시스템의 적용 가능성을 평가할 계획이다.
마지막으로, 모델의 경량화와 오픈소스 공개를 통해 연구의 확장성과 실용성을 높이고자 한다. TensorRT나 ONNX 기반 최적화 기술을 적용하여 임베디드 시스템에서도 원활히 동작할 수 있도록 개선하며, GitHub 등 오픈소스 플랫폼을 통해 프로젝트를 공개하여 외부 기여자들과의 공동 개발 체계를 구축한다. 장기적으로는 교통경찰 수신호 인식을 넘어, 보행자나 긴급 요원의 수신호, 수화 등 다양한 인간 제스처 인식 기술로 확장하여 차량-인간 간 상호작용(V2H: Vehicle to Human)을 실현하는 방향으로 발전시킬 것이다.
이러한 후속 연구와 시스템 개선을 통해, 본 프로젝트는 자율주행 기술이 인간 중심의 교통 환경과 조화롭게 공존할 수 있는 기반을 마련하고, 미래형 지능 교통체계(Intelligent Transportation System)의 핵심 구성요소로 자리매김할 것으로 기대된다.

7. 출처
Wang, Z., Chen, F., & Hu, K. (2024). A lightweight and efficient gesture recognizer for traffic police commands using spatiotemporal feature fusion. Pattern Recognition Letters, 180, 25–33. https://pubmed.ncbi.nlm.nih.gov/40415045/

